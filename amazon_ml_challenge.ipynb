{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8b6743f3",
      "metadata": {
        "id": "8b6743f3",
        "papermill": {
          "duration": 0.01573,
          "end_time": "2024-09-18T06:20:05.159672",
          "exception": false,
          "start_time": "2024-09-18T06:20:05.143942",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "**1. Data Analysyis** - Handling Values of Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c72cf6c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-09-18T06:20:05.190575Z",
          "iopub.status.busy": "2024-09-18T06:20:05.189971Z",
          "iopub.status.idle": "2024-09-18T06:20:05.592204Z",
          "shell.execute_reply": "2024-09-18T06:20:05.590638Z"
        },
        "id": "6c72cf6c",
        "papermill": {
          "duration": 0.420283,
          "end_time": "2024-09-18T06:20:05.594545",
          "exception": true,
          "start_time": "2024-09-18T06:20:05.174262",
          "status": "failed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# prompt: mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5b8ef35",
      "metadata": {
        "id": "c5b8ef35",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/amazon/dataset/test.csv\")#train.csv path\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/amazon/dataset/train.csv\")#test.csv path\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b60e90",
      "metadata": {
        "id": "51b60e90",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(test_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bb7adf9",
      "metadata": {
        "id": "2bb7adf9",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(train_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f57e87",
      "metadata": {
        "id": "00f57e87",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(test_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea27e88",
      "metadata": {
        "id": "0ea27e88",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(train_df[\"entity_name\"].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a678d019",
      "metadata": {
        "id": "a678d019",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "Installing Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2be5dec",
      "metadata": {
        "collapsed": true,
        "id": "f2be5dec",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install boto3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95b5977b",
      "metadata": {
        "collapsed": true,
        "id": "95b5977b",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4faee14",
      "metadata": {
        "collapsed": true,
        "id": "b4faee14",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install lightgbm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795cd138",
      "metadata": {
        "collapsed": true,
        "id": "795cd138",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b42e32b3",
      "metadata": {
        "collapsed": true,
        "id": "b42e32b3",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3109377",
      "metadata": {
        "collapsed": true,
        "id": "c3109377",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae944a15",
      "metadata": {
        "collapsed": true,
        "id": "ae944a15",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a660188",
      "metadata": {
        "collapsed": true,
        "id": "9a660188",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install s3fs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9396129",
      "metadata": {
        "id": "c9396129",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import tensorflow as tf\n",
        "import lightgbm as lgb\n",
        "import requests\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tqdm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d65728b",
      "metadata": {
        "id": "3d65728b",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "**2. Data Preprocessing** - Loading images with AWS S3 buckets coz large data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6c7ed5",
      "metadata": {
        "id": "aa6c7ed5",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "\n",
        "# Initialize S3 client with specific credentials\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id='Your-access',\n",
        "    aws_secret_access_key='Your-secret access',\n",
        "    region_name='us-east-1'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62dd08b9",
      "metadata": {
        "id": "62dd08b9",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "Downloading images with image url from each row at train.csv data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd33611",
      "metadata": {
        "collapsed": true,
        "id": "8dd33611",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize S3 client with specific credentials\n",
        "# Ensure the credentials have read access to the bucket and the file.\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id='Your-access',\n",
        "    aws_secret_access_key='Your-secret access',\n",
        "    region_name='us-east-1'\n",
        ")\n",
        "bucket_name = 'Your bucket name'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists('train_images'):\n",
        "    os.makedirs('train_images')\n",
        "\n",
        "# Load train.csv\n",
        "train_df = pd.read_csv(f's3://{bucket_name}/train.csv', storage_options={\"key\": \"Your-access\", \"secret\": \"Your-secret access\"})\n",
        "\n",
        "# Download images and upload to S3\n",
        "for idx, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    img_url = row['image_link']\n",
        "    img_name = f\"train_images/{row['group_id']}_{idx}.jpg\"\n",
        "\n",
        "    try:\n",
        "        # Download the image\n",
        "        img_data = requests.get(img_url).content\n",
        "\n",
        "        # Save the image to local directory\n",
        "        with open(img_name, 'wb') as handler:\n",
        "            handler.write(img_data)\n",
        "\n",
        "        # Upload the image to S3\n",
        "        s3.upload_file(img_name, bucket_name, img_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading {img_url}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d33284",
      "metadata": {
        "id": "f3d33284",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "**3. Feature Extraction based on ResNet50**  - which can be fed into a classifier (like LightGBM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc5ea48b",
      "metadata": {
        "id": "fc5ea48b",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load ResNet50 without the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Add global pooling layer to flatten the features\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "model = Model(inputs=base_model.input, outputs=x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b152f113",
      "metadata": {
        "id": "b152f113",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import boto3\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from io import BytesIO\n",
        "\n",
        "# S3 client to access images\n",
        "s3 = boto3.client('s3',\n",
        "                  aws_access_key_id='Your-access',\n",
        "                  aws_secret_access_key='Your-secret access')\n",
        "\n",
        "bucket_name = 'Your bucket name'\n",
        "prefix = 'train_images/'  # Folder where images are stored\n",
        "\n",
        "# Function to list all files in the specified S3 bucket folder\n",
        "def list_files_in_s3_folder(bucket_name, prefix):\n",
        "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
        "    if 'Contents' in response:\n",
        "        files = [obj['Key'] for obj in response['Contents']]\n",
        "        return files\n",
        "    else:\n",
        "        print(f\"No files found in {bucket_name}/{prefix}\")\n",
        "        return []\n",
        "\n",
        "# Function to load image from S3\n",
        "def load_image_from_s3(image_path, target_size=(224, 224)):\n",
        "    try:\n",
        "        img_obj = s3.get_object(Bucket=bucket_name, Key=image_path)\n",
        "        img = load_img(BytesIO(img_obj['Body'].read()), target_size=target_size)\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = img_array / 255.0  # normalize\n",
        "        return img_array\n",
        "    except s3.exceptions.NoSuchKey as e:\n",
        "        print(f\"Error: {image_path} not found in bucket {bucket_name}\")\n",
        "        return None\n",
        "\n",
        "# List all files in the 'train_images' folder\n",
        "files_in_s3 = list_files_in_s3_folder(bucket_name, prefix)\n",
        "\n",
        "# Test loading an image if the file exists\n",
        "test_image_path = 'train_images/107694_10032.jpg'  # Replace with actual image path\n",
        "\n",
        "if test_image_path in files_in_s3:\n",
        "    sample_image = load_image_from_s3(test_image_path)\n",
        "    if sample_image is not None:\n",
        "        print(f\"Loaded image: {test_image_path}, shape: {sample_image.shape}\")\n",
        "else:\n",
        "    print(f\"Image {test_image_path} does not exist in S3 bucket {bucket_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e8c29a",
      "metadata": {
        "collapsed": true,
        "id": "72e8c29a",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from io import BytesIO\n",
        "\n",
        "# S3 client to access images\n",
        "s3 = boto3.client('s3',\n",
        "                  aws_access_key_id='Your-access',\n",
        "                  aws_secret_access_key='Your-secret access')\n",
        "\n",
        "bucket_name = 'bucket name'\n",
        "prefix = 'train_images/'  # Folder where images are stored\n",
        "\n",
        "# Load train.csv\n",
        "train_df = pd.read_csv('s3://bucket name/train.csv')\n",
        "\n",
        "# Load image from S3\n",
        "def load_image_from_s3(image_path, target_size=(224, 224)):\n",
        "    img_obj = s3.get_object(Bucket=bucket_name, Key=image_path)\n",
        "    img = load_img(BytesIO(img_obj['Body'].read()), target_size=target_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = img_array / 255.0  # normalize\n",
        "    return img_array\n",
        "\n",
        "# Prepare image paths for the training data\n",
        "def prepare_images(image_paths):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        img = load_image_from_s3(path)\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# ResNet50 Feature Extractor\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Prepare image paths for training data\n",
        "image_paths = [prefix + str(group_id) + '_' + str(idx) + '.jpg' for idx, group_id in enumerate(train_df['group_id'])]\n",
        "\n",
        "# Preprocess images\n",
        "train_images = prepare_images(image_paths)\n",
        "\n",
        "# Extract features from the images\n",
        "image_features = model.predict(train_images)\n",
        "\n",
        "# Flatten and convert features to array\n",
        "image_features = np.reshape(image_features, (image_features.shape[0], -1))\n",
        "\n",
        "# Now map the features to entity_value (target labels)\n",
        "train_labels = train_df['entity_value'].values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5538d6e6",
      "metadata": {
        "id": "5538d6e6",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(image_paths[:5])  # Print first 5 image paths to check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b95d64d",
      "metadata": {
        "id": "5b95d64d",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Test saving an image from S3\n",
        "save_image_from_s3('train_images/748919_0.jpg', 'test_image.jpg')\n",
        "\n",
        "# Open the saved image file and verify it's correctly loaded\n",
        "from PIL import Image\n",
        "img = Image.open('test_image.jpg')\n",
        "img.show()  # This should display the image if it's valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e2147e",
      "metadata": {
        "id": "35e2147e",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def load_image_from_s3(image_path, target_size=(224, 224)):\n",
        "    try:\n",
        "        img_obj = s3.get_object(Bucket=bucket_name, Key=image_path)\n",
        "        img_data = img_obj['Body'].read()\n",
        "        print(f\"Image data length: {len(img_data)} bytes\")  # Check the size of the image data\n",
        "        img = load_img(BytesIO(img_data), target_size=target_size)\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = img_array / 255.0  # Normalize\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfadad45",
      "metadata": {
        "id": "dfadad45",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "**4. Feature Storage** - Batch processing & feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e1939db",
      "metadata": {
        "collapsed": true,
        "id": "8e1939db",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def process_images_in_batches(image_paths, batch_size=32):\n",
        "    features = []\n",
        "    start_time = time.time()  # Start time\n",
        "    for batch_start in range(0, len(image_paths), batch_size):\n",
        "        batch_paths = image_paths[batch_start:batch_start + batch_size]\n",
        "        print(f\"Processing batch {batch_start // batch_size + 1} of size {len(batch_paths)}\")  # Debugging\n",
        "        batch_images = prepare_images(batch_paths)\n",
        "        if len(batch_images) > 0:\n",
        "            batch_features = model.predict(batch_images)\n",
        "            features.append(batch_features)\n",
        "    end_time = time.time()  # End time\n",
        "    print(f\"Total processing time: {end_time - start_time} seconds\")\n",
        "    return np.vstack(features)\n",
        "\n",
        "# Process images in batches\n",
        "train_images = process_images_in_batches(image_paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eef1031e",
      "metadata": {
        "id": "eef1031e",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def process_images_in_batches(image_paths, batch_size=32):\n",
        "    features = []\n",
        "    start_time = time.time()  # Start time\n",
        "    for batch_start in range(0, len(image_paths), batch_size):\n",
        "        batch_paths = image_paths[batch_start:batch_start + batch_size]\n",
        "        print(f\"Processing batch {batch_start // batch_size + 1} of size {len(batch_paths)}\")  # Debugging\n",
        "        batch_images = prepare_images(batch_paths)\n",
        "        print(f\"Batch images shape: {batch_images.shape if batch_images is not None else 'None'}\")\n",
        "        if len(batch_images) > 0:\n",
        "            batch_features = model.predict(batch_images)\n",
        "            print(f\"Batch features shape: {batch_features.shape}\")\n",
        "            features.append(batch_features)\n",
        "    end_time = time.time()  # End time\n",
        "    print(f\"Total processing time: {end_time - start_time} seconds\")\n",
        "    return np.vstack(features) if len(features) > 0 else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f92ffc",
      "metadata": {
        "id": "b7f92ffc",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_labels = train_df['entity_value'].values  # Assuming they are numerical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84456d56",
      "metadata": {
        "id": "84456d56",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def prepare_images(image_paths):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        img = load_image_from_s3(path)\n",
        "        if img is not None:\n",
        "            print(f\"Image shape: {img.shape}\")  # Check image dimensions\n",
        "            images.append(img)\n",
        "    return np.array(images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5228fe1a",
      "metadata": {
        "id": "5228fe1a",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "prefix = 'train_images/'  # Your folder path in S3\n",
        "image_paths = [prefix + str(group_id) + '_' + str(idx) + '.jpg' for idx, group_id in enumerate(train_df['group_id'])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "709ed969",
      "metadata": {
        "id": "709ed969",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('s3://bucket name/train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf96ef64",
      "metadata": {
        "id": "cf96ef64",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(image_paths[:5])  # Check first 5 image paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd7ac06e",
      "metadata": {
        "id": "fd7ac06e",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def test_load_image_from_s3(image_path, target_size=(224, 224)):\n",
        "    try:\n",
        "        img_obj = s3.get_object(Bucket=bucket_name, Key=image_path)\n",
        "        img = load_img(BytesIO(img_obj['Body'].read()), target_size=target_size)\n",
        "        img.show()  # Display image to ensure it’s valid\n",
        "        print(f\"Successfully loaded image: {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "\n",
        "# Test with a specific image path\n",
        "test_load_image_from_s3('train_images/748919_0.jpg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33969bc1",
      "metadata": {
        "id": "33969bc1",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def prepare_images(image_paths):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        try:\n",
        "            img = load_image_from_s3(path)\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {path}: {e}\")\n",
        "    return np.array(images) if len(images) > 0 else None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16c26e2a",
      "metadata": {
        "id": "16c26e2a",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "def process_images_in_batches(image_paths, batch_size=32):\n",
        "    features = []\n",
        "    start_time = time.time()  # Start time\n",
        "    for batch_start in range(0, len(image_paths), batch_size):\n",
        "        batch_paths = image_paths[batch_start:batch_start + batch_size]\n",
        "        print(f\"Processing batch {batch_start // batch_size + 1} of size {len(batch_paths)}\")  # Debugging\n",
        "        batch_images = prepare_images(batch_paths)\n",
        "        if batch_images is not None and len(batch_images) > 0:\n",
        "            batch_features = model.predict(batch_images)\n",
        "            print(f\"Batch features shape: {batch_features.shape}\")\n",
        "            features.append(batch_features)\n",
        "    end_time = time.time()  # End time\n",
        "    print(f\"Total processing time: {end_time - start_time} seconds\")\n",
        "    return np.vstack(features) if len(features) > 0 else None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b34c92",
      "metadata": {
        "id": "c5b34c92",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "**5. Train a Model to Map Features to Entity Values - Light GBM**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c07efa4c",
      "metadata": {
        "collapsed": true,
        "id": "c07efa4c",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import boto3\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from io import BytesIO\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# AWS S3 client setup\n",
        "s3 = boto3.client('s3',\n",
        "                  aws_access_key_id='Your-access',\n",
        "                  aws_secret_access_key='Your-secret access')\n",
        "\n",
        "bucket_name = 'bucket name'\n",
        "prefix = 'train_images/'  # Folder where images are stored\n",
        "\n",
        "# Load CSV from S3\n",
        "train_df = pd.read_csv('s3://bucket name/train.csv')\n",
        "\n",
        "# Load image from S3\n",
        "def load_image_from_s3(image_path, target_size=(224, 224)):\n",
        "    try:\n",
        "        img_obj = s3.get_object(Bucket=bucket_name, Key=image_path)\n",
        "        img = load_img(BytesIO(img_obj['Body'].read()), target_size=target_size)\n",
        "        img_array = img_to_array(img)\n",
        "        img_array = img_array / 255.0  # normalize\n",
        "        return img_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Prepare image paths\n",
        "def prepare_images(image_paths):\n",
        "    images = []\n",
        "    for path in image_paths:\n",
        "        img = load_image_from_s3(path)\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# ResNet50 Feature Extractor\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "# Prepare image paths\n",
        "image_paths = [prefix + str(group_id) + '_' + str(idx) + '.jpg' for idx, group_id in enumerate(train_df['group_id'])]\n",
        "\n",
        "# Prepare images and extract features\n",
        "train_images = prepare_images(image_paths)\n",
        "\n",
        "# Ensure features are extracted properly\n",
        "if train_images is not None and len(train_images) > 0:\n",
        "    # Extract features\n",
        "    image_features = model.predict(train_images)\n",
        "\n",
        "    # Flatten features\n",
        "    image_features = np.reshape(image_features, (image_features.shape[0], -1))\n",
        "\n",
        "    # Map the features to entity_value (target labels)\n",
        "    train_labels = train_df['entity_value'].values\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(image_features, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Prepare data for LightGBM\n",
        "    train_data = lgb.Dataset(X_train, label=y_train)\n",
        "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "    # Train the model\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'l2',\n",
        "        'boosting_type': 'gbdt',\n",
        "    }\n",
        "    gbm = lgb.train(params, train_data, valid_sets=[val_data], num_boost_round=100)\n",
        "\n",
        "    # Predict on validation set\n",
        "    y_pred = gbm.predict(X_val, num_iteration=gbm.best_iteration)\n",
        "\n",
        "    # Save results to CSV\n",
        "    output_df = pd.DataFrame({'index': range(len(y_pred)), 'prediction': y_pred})\n",
        "    output_df.to_csv('validation_predictions.csv', index=False)\n",
        "\n",
        "else:\n",
        "    print(\"No features extracted. Please check the processing steps.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e8bdc6",
      "metadata": {
        "collapsed": true,
        "id": "b3e8bdc6",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(train_images.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4413f29f",
      "metadata": {
        "collapsed": true,
        "id": "4413f29f",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_images, train_labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8c07f9",
      "metadata": {
        "collapsed": true,
        "id": "df8c07f9",
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train LightGBM Model (or any regression model)\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Prepare data for LightGBM\n",
        "train_data = lgb.Dataset(image_features, label=train_labels)\n",
        "\n",
        "# Train the model\n",
        "params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'rmse',\n",
        "    'boosting': 'gbdt',\n",
        "    'learning_rate': 0.01,\n",
        "    'num_leaves': 31\n",
        "}\n",
        "model_lgb = lgb.train(params, train_data, num_boost_round=100)\n",
        "\n",
        "# Load test.csv and prepare test image paths\n",
        "test_df = pd.read_csv(f's3://{bucket_name}/test.csv')\n",
        "test_image_paths = [prefix + str(group_id) + '_' + str(idx) + '.jpg' for idx, group_id in enumerate(test_df['group_id'])]\n",
        "\n",
        "# Preprocess test images\n",
        "test_images = prepare_images(test_image_paths)\n",
        "\n",
        "# Extract features from the test images\n",
        "test_image_features = model.predict(test_images)\n",
        "\n",
        "# Flatten and convert test features to array\n",
        "test_image_features = np.reshape(test_image_features, (test_image_features.shape[0], -1))\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model_lgb.predict(test_image_features)\n",
        "\n",
        "# Save predictions to CSV\n",
        "output_df = pd.DataFrame({'index': test_df['index'], 'prediction': predictions})\n",
        "output_df.to_csv('predictions.csv', index=False)\n",
        "\n",
        "# Optionally upload the predictions CSV to S3\n",
        "s3.upload_file('predictions.csv', bucket_name, 'predictions.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ab88ca1",
      "metadata": {
        "id": "8ab88ca1",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "**6. Evaluation - F1 Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184dc79c",
      "metadata": {
        "id": "184dc79c",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = f1_score(y_true, y_pred, average='macro')  # or 'weighted'\n",
        "print(f\"F1 Score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5975cccf",
      "metadata": {
        "id": "5975cccf",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Predict on validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Get the predicted class labels\n",
        "y_pred_labels = np.argmax(y_pred, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b85adfd",
      "metadata": {
        "id": "6b85adfd",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_val, y_pred_labels)\n",
        "print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_val, y_pred_labels)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8e6f68",
      "metadata": {
        "id": "0d8e6f68",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "source": [
        "**7. Save Prediction into csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfb96f33",
      "metadata": {
        "id": "bfb96f33",
        "papermill": {
          "duration": null,
          "end_time": null,
          "exception": null,
          "start_time": null,
          "status": "pending"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming you have predictions for your test data\n",
        "test_predictions = model.predict(X_test)  # Replace X_test with actual test feature data\n",
        "test_predictions_labels = np.argmax(test_predictions, axis=1)\n",
        "\n",
        "# Create a DataFrame with predictions\n",
        "submission_df = pd.DataFrame({\n",
        "    'image_id': test_image_ids,  # Replace with actual image IDs\n",
        "    'prediction': test_predictions_labels\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv('predictions.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5724677,
          "sourceId": 9424457,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4.058381,
      "end_time": "2024-09-18T06:20:06.030169",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-09-18T06:20:01.971788",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}